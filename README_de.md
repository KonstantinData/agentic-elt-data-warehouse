# Agentic ELT Data Warehouse

ğŸ‡ºğŸ‡¸ **[English Version](README.md)**

Dieses Repository enthÃ¤lt eine produktionsreife agentic ELT/Analytics-Pipeline, die LLM-Agents verwendet, um automatisch Datentransformationscode zu generieren und auszufÃ¼hren. Das System demonstriert, wie KI in traditionelle Data Engineering Workflows integriert werden kann, um sich selbst anpassende Datenpipelines zu erstellen.

## ğŸš€ Schnellstart

### Fork & Clone
1. Forken Sie dieses Repository zu Ihrem GitHub-Account
2. Klonen Sie Ihren Fork lokal:
```bash
git clone https://github.com/YOUR_USERNAME/agentic-elt-data-warehouse.git
cd agentic-elt-data-warehouse
```

### Voraussetzungen
- **Python 3.8+** (getestet mit Python 3.12)
- **OpenAI API Key** (fÃ¼r LLM-Agents)
- **Git** fÃ¼r Versionskontrolle

### Installation

1. **Virtuelle Umgebung erstellen:**
```bash
python -m venv venv
# Windows
venv\Scripts\activate
# Linux/Mac
source venv/bin/activate
```

2. **AbhÃ¤ngigkeiten installieren:**
```bash
pip install -r requirements.txt
```

3. **Umgebungsvariablen einrichten:**
```bash
# Beispiel-Umgebungsdatei kopieren
cp configs\.env.example .env
# .env bearbeiten und OpenAI API Key hinzufÃ¼gen
# OPENAI_API_KEY=your_api_key_here
```

### Beispieldatensatz
Das Repository enthÃ¤lt einen vollstÃ¤ndigen Beispieldatensatz im `raw/` Verzeichnis:
- **CRM-Daten** (`raw/source_crm/`): Kundeninformationen, Produktdetails, Verkaufsdaten
- **ERP-Daten** (`raw/source_erp/`): ZusÃ¤tzliche Kundendaten, Standortinformationen, Produktkategorien, Transaktionen

Dieser synthetische Datensatz reprÃ¤sentiert die Datenstruktur eines typischen mittelstÃ¤ndischen Unternehmens und ist sofort einsatzbereit.

## ğŸƒâ™‚ï¸ Pipeline ausfÃ¼hren

FÃ¼hren Sie die komplette ELT-Pipeline aus:
```bash
python .\src\runs\start_run.py
```

### Was passiert
Die Pipeline fÃ¼hrt diese Schritte automatisch aus:

1. **ğŸ¥‰ Bronze Layer** - Rohdatenaufnahme
   - Kopiert CSV-Dateien aus `raw/` Verzeichnissen
   - Validiert DatenintegritÃ¤t mit Checksummen
   - Erstellt unverÃ¤nderliche Snapshots

2. **ğŸ¥ˆ Silver Layer** - Datenbereinigung & Standardisierung
   - LLM-Agents analysieren DatenqualitÃ¤tsprobleme
   - Generieren Python-Code fÃ¼r Datenbereinigung
   - FÃ¼hren Transformationen automatisch aus
   - Behandeln fehlende Werte, Datentypen, Formatierung

3. **ğŸ¥‡ Gold Layer** - Business Marts Erstellung
   - LLM-Agents entwerfen Star-Schema
   - Generieren Dimensions- und Fact-Tabellen
   - Erstellen Business-KPI-Aggregationen
   - Bauen analytics-bereite DatensÃ¤tze

4. **ğŸ“Š Zusammenfassungsbericht** - AusfÃ¼hrungszusammenfassung
   - Pipeline-AusfÃ¼hrungsmetriken
   - DatenqualitÃ¤tsbewertungen
   - Generierte Code-Dokumentation

## ğŸ“ˆ Dashboard ausfÃ¼hren

### Streamlit Ãœberblick

Das multipage Dashboard basiert auf Streamlit (`src/dashboard/app.py`) und lÃ¤dt die neuesten Silver-Artefakte aus `artifacts/runs/<run_id>/silver/data/`. Es listet alle verfÃ¼gbaren Runs, speichert die Sidebar-Filter (Datum, Produktlinien, LÃ¤nder, Geschlecht usw.) im Session-State und lÃ¤dt die DatensÃ¤tze neu, sobald ein anderer Run gewÃ¤hlt wird. Jede Seite â€“ vom Executive Overview bis zu Diagnosen und Exporten â€“ greift auf gemeinsame Komponenten zurÃ¼ck, um Filter zu persistieren und den aktuellen Kontext zum Download bereitzustellen.

### Dashboard starten

Starten Sie das Dashboard, sobald mindestens ein Silver-Run abgeschlossen ist:

#### Windows

```powershell
scripts\run_dashboard.ps1
```

#### Linux/macOS

```bash
./scripts/run_dashboard.sh
```

Alternativ kÃ¶nnen Sie Streamlit direkt starten, um CLI-Flags mitzugeben:

```bash
python -m streamlit run src/dashboard/app.py
```

Das Dashboard entdeckt Artefakte unter `artifacts/runs/<run_id>/silver/data/` (mit sicherer RÃ¼ckfallebene zu `artifacts/silver/<run_id>/data/`). Fehlen Runs, fordert es Sie auf, zuerst die Pipeline zu starten. Der zuerst gewÃ¤hlte Run ist immer der aktuellste Abschluss, aber Sie kÃ¶nnen beliebig Ã¤ltere Runs zum Vergleich auswÃ¤hlen.

### Streamlit-Einblicke

- **Executive Overview (pages/01)** zeigt KPI-Karten, Trenddiagramme, Produktmix-Maps, Order-Signale und das Diagnostik-Panel, das die wichtigsten Kennzahlen des gewÃ¤hlten Silver-Runs zusammenfÃ¼hrt.
- **Exploration Sandbox (pages/02)** bietet eine interaktive Vorschau auf die gefilterten Daten sowie Tabellen zu Top-Produkten, Top-LÃ¤ndern und Top-Kunden, damit Sie Transformationslogik direkt im Dashboard Ã¼berprÃ¼fen kÃ¶nnen.
- **Run Diagnostics (pages/03)** spiegelt Metadaten, Log-Informationen und fehlende Werte wider, die die Pipeline erfasst, und stellt sie in einem aufklappbaren Panel dar.
- **Exports & Context (pages/04)** erlaubt den Download des gefilterten Datensatzes als CSV sowie eines JSON-Pakets mit aktiven Filtern, Run-ID und Artefaktquelle fÃ¼r Audit-Zwecke.
- Die Sidebar-Filter werden Ã¼ber `src/dashboard/components/filters.py` konfiguriert und bleiben so lange erhalten, bis Sie sie zurÃ¼cksetzen.

### Streamlit-Konfiguration und Tipps

- `.streamlit/config.toml` definiert die Light/Dark-Themen und kann angepasst werden, wenn Sie ein eigenes Farbschema benÃ¶tigen.
- CLI-Optionen lassen sich Ã¼ber Umgebungsvariablen wie `STREAMLIT_SERVER_PORT` oder `STREAMLIT_BROWSER_GATHER_USAGE_STATS` steuern, etwa wenn der Standard-Port blockiert ist oder Sie die Nutzungsstatistik deaktivieren wollen.
- Bevor Sie den Streamlit-Dashboard neu starten, fÃ¼hren Sie `python .\src\runs\start_run.py` (oder den Orchestrator) erneut aus, damit frische Silver-Artefakte vorliegen; veraltete Runs zeigen stattdessen erklÃ¤renden Hinweistext.
- Verwenden Sie die Skripte `scripts/run_dashboard.ps1`/`.sh`, um den Befehl Ã¼ber Betriebssysteme hinweg konsistent zu halten, oder geben Sie direkt Flags wie `--server.headless true` oder `--server.address` an `python -m streamlit run`, wenn Sie das Dashboard hosten.

## ğŸ“ Ausgabestruktur

Alle Pipeline-Ausgaben sind nach Run-ID organisiert:

```
artifacts/
â”œâ”€â”€ bronze/YYYYMMDD_HHMMSS_#hash/     # Rohdaten-Snapshots
â”‚   â”œâ”€â”€ data/*.csv                     # Kopierte Quelldateien
â”‚   â””â”€â”€ reports/elt_report.html        # Aufnahmebericht
â”œâ”€â”€ silver/YYYYMMDD_HHMMSS_#hash/      # Bereinigte Daten
â”‚   â”œâ”€â”€ data/*.csv                     # Standardisierte Tabellen
â”‚   â””â”€â”€ reports/elt_report.html        # QualitÃ¤tsbericht
â”œâ”€â”€ gold/marts/YYYYMMDD_HHMMSS_#hash/  # Business Marts
â”‚   â”œâ”€â”€ data/*.csv                     # Star-Schema-Tabellen
â”‚   â””â”€â”€ reports/gold_report.html       # Marts-Dokumentation
â”œâ”€â”€ orchestrator/YYYYMMDD_HHMMSS_#hash/# AusfÃ¼hrungslogs
â”‚   â””â”€â”€ logs/*.log                     # Detaillierte Schritt-Logs
â””â”€â”€ reports/YYYYMMDD_HHMMSS_#hash/     # Zusammenfassungsberichte
    â”œâ”€â”€ summary_report.md              # Menschenlesbare Zusammenfassung
    â””â”€â”€ summary_report.json            # Maschinenlesbare Metriken
```

## ğŸ”§ Konfigurationsoptionen

### Inkrementelle Verarbeitung
Die Pipeline erkennt automatisch unverÃ¤nderte Rohdaten und Ã¼berspringt die Verarbeitung, wenn keine neuen Daten verfÃ¼gbar sind. Dies spart Zeit und Ressourcen bei nachfolgenden LÃ¤ufen mit identischen Eingabedateien.

### Umgebungsvariablen
Wichtige Konfiguration in `.env`:
```bash
OPENAI_API_KEY=your_key_here          # Erforderlich fÃ¼r LLM-Agents
ORCHESTRATOR_RUN_ID=custom_run_id     # Optional: benutzerdefinierte Run-ID
```

## ğŸ§ª Testen

FÃ¼hren Sie die komplette Test-Suite aus:
```bash
pytest -q
```

Test-Kategorien:
- **Unit-Tests** - Einzelkomponenten-Tests
- **Integrationstests** - End-to-End-Pipeline-Validierung
- **Vertragstests** - Datenschema-Validierung
- **QualitÃ¤tstests** - Code-QualitÃ¤t und Dokumentation

## ğŸ—ï¸ Architektur

### Agentic-Komponenten
- **Draft Agents** - Analysieren Daten und generieren Transformationscode
- **Builder Agents** - Verfeinern und optimieren generierten Code
- **Quality Agents** - Validieren Code-QualitÃ¤t und Performance

### Datenfluss
```
Rohdaten â†’ Bronze (Aufnahme) â†’ Silver (Bereinigung) â†’ Gold (Business-Logik) â†’ Berichte
     â†“           â†“                    â†“                    â†“
   LLM-Analyse â†’ Code-Generierung â†’ AusfÃ¼hrung â†’ Validierung
```

### Hauptmerkmale
- **Deterministische AusfÃ¼hrung** - Gleiche Eingaben produzieren identische Ausgaben
- **Audit-Trail** - VollstÃ¤ndige Lineage-Verfolgung
- **Fehlerbehandlung** - Elegante Fehlerwiederherstellung
- **Inkrementelle Verarbeitung** - Ãœberspringen unverÃ¤nderter Daten
- **DSGVO-KonformitÃ¤t** - PII-Behandlung und Pseudonymisierung

## ğŸ“Š Beispieldaten-Ãœbersicht

Der enthaltene Datensatz simuliert:
- **~1000 Kunden** Ã¼ber mehrere Segmente
- **~50 Produkte** in verschiedenen Kategorien
- **~5000 Verkaufstransaktionen** Ã¼ber ZeitrÃ¤ume
- **Mehrere DatenqualitÃ¤tsprobleme** zum Testen der Bereinigungslogik

Daten enthalten absichtliche QualitÃ¤tsprobleme:
- Fehlende Werte
- Inkonsistente Formatierung
- Doppelte DatensÃ¤tze
- Datentyp-Unstimmigkeiten

## ğŸ” Monitoring & Observability

Jeder Lauf generiert umfassende Monitoring-Daten:
- **AusfÃ¼hrungsmetriken** - Laufzeit, Speicherverbrauch, Erfolgsraten
- **DatenqualitÃ¤ts-Scores** - VollstÃ¤ndigkeit, GÃ¼ltigkeit, Konsistenz
- **Code-Generierungslogs** - LLM-Interaktionen und Entscheidungen
- **Fehler-Tracking** - Detaillierte Fehleranalyse

## ğŸ¤ Mitwirken

1. Repository forken
2. Feature-Branch erstellen: `git checkout -b feature/amazing-feature`
3. Ã„nderungen committen: `git commit -m 'Add amazing feature'`
4. Zu Branch pushen: `git push origin feature/amazing-feature`
5. Pull Request Ã¶ffnen

## ğŸ“ Lizenz

Dieses Projekt ist unter der MIT-Lizenz lizenziert - siehe die [LICENSE](LICENSE) Datei fÃ¼r Details.

## ğŸ†˜ Fehlerbehebung

### HÃ¤ufige Probleme

**Fehlender OpenAI API Key:**
```
RuntimeError: Missing OPEN_AI_KEY or OPENAI_API_KEY in .env
```
LÃ¶sung: OpenAI API Key zur `.env` Datei hinzufÃ¼gen

**Import-Fehler:**
```
ModuleNotFoundError: No module named 'xyz'
```
LÃ¶sung: Sicherstellen, dass virtuelle Umgebung aktiviert und AbhÃ¤ngigkeiten installiert sind

**Berechtigungsfehler:**
```
PermissionError: [Errno 13] Permission denied
```
LÃ¶sung: Dateiberechtigungen prÃ¼fen und Schreibzugriff auf `artifacts/` Verzeichnis sicherstellen

### Hilfe erhalten
- Bestehende [Issues](https://github.com/YOUR_USERNAME/agentic-elt-data-warehouse/issues) prÃ¼fen
- AusfÃ¼hrungslogs in `artifacts/orchestrator/*/logs/` Ã¼berprÃ¼fen
- Debug-Logging durch Setzen von `LOG_LEVEL=DEBUG` in `.env` aktivieren

---

**Bereit, KI-gestÃ¼tzte Datentechnik in Aktion zu sehen? FÃ¼hren Sie `python .\src\runs\start_run.py` aus und erleben Sie die Magie! âœ¨**
